{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The feature engineering was redone starting from the original work and with extra work by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#updates:\n",
    "#applied imputation into numerical and categorical data to improve the score by 0.327, now 0.9194\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import stats\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "\n",
    "import gc\n",
    "import os \n",
    "\n",
    "#print where those datasets are located \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "#df_trans = pd.read_csv('train_transaction.csv')\n",
    "df_trans = pd.read_csv('train_transaction.csv',index_col='TransactionID')\n",
    "df_test_trans = pd.read_csv('test_transaction.csv',index_col='TransactionID')\n",
    "\n",
    "#record index of test datasets\n",
    "test_trans_id = df_test_trans.index\n",
    "\n",
    "#df_id = pd.read_csv('train_identity.csv')\n",
    "df_id = pd.read_csv('train_identity.csv',index_col='TransactionID')\n",
    "df_test_id = pd.read_csv('test_identity.csv',index_col='TransactionID')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_split(dataframe):\n",
    "    \n",
    "    #expand = split and expand to seperate column\n",
    "    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "\n",
    "    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n",
    "    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n",
    "\n",
    "    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n",
    "    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n",
    "    \n",
    "    #very specifically group device brands to help making decision\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    #make the device name that are less than 200 to be other\n",
    "    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    \n",
    "    #new column to record that this device has id???\n",
    "    dataframe['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#C sum feature\n",
    "df_trans[\"C_sum\"] = df_trans.loc[:,'C1':'C14'].sum(axis = 1).astype(np.int8)\n",
    "df_test_trans[\"C_sum\"] = df_test_trans.loc[:,'C1':'C14'].sum(axis = 1).astype(np.int8)\n",
    "\n",
    "    \n",
    "##D1 - D15\n",
    "df_trans['D_na'] =df_trans.loc[:,'D1':'D14'].isna().sum(axis=1).astype(np.int8)\n",
    "df_test_trans['D_na'] =df_test_trans.loc[:,'D1':'D14'].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "    \n",
    "##M1-9\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "df_trans['M_na'] = df_trans[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "df_test_trans['M_na'] = df_test_trans[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "    \n",
    "##V_ features\n",
    "df_trans['V_na'] =df_trans.loc[:,\"V1\":\"V339\"].isna().sum(axis=1).astype(np.int8)\n",
    "df_test_trans['V_na'] =df_test_trans.loc[:,\"V1\":\"V339\"].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "\n",
    "##ids \n",
    "df_id['id_na'] = df_id.loc[:,\"id_01\":\"id_38\"].isna().sum(axis=1).astype(np.int8)\n",
    "df_test_id['id_na'] = df_test_id.loc[:,\"id_01\":\"id_38\"].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = id_split(df_id)\n",
    "df_test_id = id_split(df_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
      "TransactionID                                                                   \n",
      "2987000              0          86400            68.5         W  13926    NaN   \n",
      "2987001              0          86401            29.0         W   2755  404.0   \n",
      "2987002              0          86469            59.0         W   4663  490.0   \n",
      "2987003              0          86499            50.0         W  18132  567.0   \n",
      "2987004              0          86506            50.0         H   4497  514.0   \n",
      "\n",
      "               card3       card4  card5   card6  ...  device_name  \\\n",
      "TransactionID                                    ...                \n",
      "2987000        150.0    discover  142.0  credit  ...          NaN   \n",
      "2987001        150.0  mastercard  102.0  credit  ...          NaN   \n",
      "2987002        150.0        visa  166.0   debit  ...          NaN   \n",
      "2987003        150.0  mastercard  117.0   debit  ...          NaN   \n",
      "2987004        150.0  mastercard  102.0  credit  ...      Samsung   \n",
      "\n",
      "               device_version  OS_id_30  version_id_30 browser_id_31  \\\n",
      "TransactionID                                                          \n",
      "2987000                   NaN       NaN            NaN           NaN   \n",
      "2987001                   NaN       NaN            NaN           NaN   \n",
      "2987002                   NaN       NaN            NaN           NaN   \n",
      "2987003                   NaN       NaN            NaN           NaN   \n",
      "2987004                NRD90M   Android            7.0       samsung   \n",
      "\n",
      "              version_id_31  screen_width  screen_height  had_id  tot_na  \n",
      "TransactionID                                                             \n",
      "2987000                 NaN           NaN            NaN     NaN     -12  \n",
      "2987001                 NaN           NaN            NaN     NaN     -16  \n",
      "2987002                 NaN           NaN            NaN     NaN     -35  \n",
      "2987003                 NaN           NaN            NaN     NaN     -19  \n",
      "2987004             browser          2220           1080     1.0    -119  \n",
      "\n",
      "[5 rows x 448 columns]\n",
      "               TransactionDT  TransactionAmt ProductCD  card1  card2  card3  \\\n",
      "TransactionID                                                                 \n",
      "3663549             18403224           31.95         W  10409  111.0  150.0   \n",
      "3663550             18403263           49.00         W   4272  111.0  150.0   \n",
      "3663551             18403310          171.00         W   4476  574.0  150.0   \n",
      "3663552             18403310          284.95         W  10989  360.0  150.0   \n",
      "3663553             18403317           67.95         W  18018  452.0  150.0   \n",
      "\n",
      "                    card4  card5  card6  addr1  ...  device_name  \\\n",
      "TransactionID                                   ...                \n",
      "3663549              visa  226.0  debit  170.0  ...          NaN   \n",
      "3663550              visa  226.0  debit  299.0  ...          NaN   \n",
      "3663551              visa  226.0  debit  472.0  ...          NaN   \n",
      "3663552              visa  166.0  debit  205.0  ...          NaN   \n",
      "3663553        mastercard  117.0  debit  264.0  ...          NaN   \n",
      "\n",
      "               device_version  OS_id_30 version_id_30 browser_id_31  \\\n",
      "TransactionID                                                         \n",
      "3663549                   NaN       NaN           NaN           NaN   \n",
      "3663550                   NaN       NaN           NaN           NaN   \n",
      "3663551                   NaN       NaN           NaN           NaN   \n",
      "3663552                   NaN       NaN           NaN           NaN   \n",
      "3663553                   NaN       NaN           NaN           NaN   \n",
      "\n",
      "               version_id_31  screen_width  screen_height  had_id  tot_na  \n",
      "TransactionID                                                              \n",
      "3663549                  NaN           NaN            NaN     NaN     -36  \n",
      "3663550                  NaN           NaN            NaN     NaN     -34  \n",
      "3663551                  NaN           NaN            NaN     NaN     -38  \n",
      "3663552                  NaN           NaN            NaN     NaN     -33  \n",
      "3663553                  NaN           NaN            NaN     NaN     -36  \n",
      "\n",
      "[5 rows x 447 columns]\n",
      "Mem. usage decreased to 689.34 Mb (65.6% reduction)\n",
      "Mem. usage decreased to 598.71 Mb (65.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##do not need to include right_index\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True,  on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, on='TransactionID')\n",
    "\n",
    "#add total na for train and test data. \n",
    "df_train['tot_na'] = df_train.isna().sum(axis=1).astype(np.int8) \n",
    "df_test['tot_na'] = df_test.isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "\n",
    "# y_train = df_train['isFraud'].copy()\n",
    "del df_trans, df_id, df_test_trans, df_test_id\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "##Deal with missing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1600'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[3577531,\"screen_height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm#\n",
    "\n",
    "useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n",
    "                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n",
    "                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n",
    "                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n",
    "                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n",
    "                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n",
    "                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n",
    "                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n",
    "                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n",
    "                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n",
    "                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n",
    "                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n",
    "                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n",
    "                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n",
    "                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n",
    "                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n",
    "                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n",
    "                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n",
    "                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n",
    "                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n",
    "                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id','tot_na','C_sum','D_na','M_na','V_na','id_na']\n",
    "print(len(useful_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in df_train.columns if col not in useful_features]\n",
    "cols_to_drop.remove('isFraud')\n",
    "cols_to_drop.remove('TransactionDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_a = ['TransactionAmt', 'id_02', 'D15']\n",
    "columns_b = ['card1', 'card4', 'addr1']\n",
    "\n",
    "for col_a in columns_a:\n",
    "    for col_b in columns_b:\n",
    "        for df in [df_train, df_test]:\n",
    "            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')\n",
    "            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount.\n",
    "df_train['TransactionAmt_Log'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt_Log'] = np.log(df_test['TransactionAmt'])\n",
    "\n",
    "# New feature - decimal part of the transaction amount.\n",
    "df_train['TransactionAmt_decimal'] = ((df_train['TransactionAmt'] - df_train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "df_test['TransactionAmt_decimal'] = ((df_test['TransactionAmt'] - df_test['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "# New feature - day of week in which a transaction happened.\n",
    "df_train['Transaction_day_of_week'] = np.floor((df_train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "df_test['Transaction_day_of_week'] = np.floor((df_test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "\n",
    "# New feature - hour of the day in which a transaction happened.\n",
    "df_train['Transaction_hour'] = np.floor(df_train['TransactionDT'] / 3600) % 24\n",
    "df_test['Transaction_hour'] = np.floor(df_test['TransactionDT'] / 3600) % 24\n",
    "\n",
    "# Some arbitrary features interaction\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    df_train[feature] = df_train[f1].astype(str) + '_' + df_train[f2].astype(str)\n",
    "    df_test[feature] = df_test[f1].astype(str) + '_' + df_test[f2].astype(str)\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(list(df_train[feature].astype(str).values) + list(df_test[feature].astype(str).values))\n",
    "    df_train[feature] = le.transform(list(df_train[feature].astype(str).values))\n",
    "    df_test[feature] = le.transform(list(df_test[feature].astype(str).values))\n",
    "\n",
    "# Encoding - count encoding for both df_train and df_test\n",
    "for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n",
    "    df_train[feature + '_count_full'] = df_train[feature].map(pd.concat([df_train[feature], df_test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "    df_test[feature + '_count_full'] = df_test[feature].map(pd.concat([df_train[feature], df_test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# Encoding - count encoding separately for df_train and df_test\n",
    "for feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n",
    "    df_train[feature + '_count_dist'] = df_train[feature].map(df_train[feature].value_counts(dropna=False))\n",
    "    df_test[feature + '_count_dist'] = df_test[feature].map(df_test[feature].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the rest \n",
    "for col in df_train.columns:\n",
    "    if df_train[col].dtype == 'object':\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(df_train[col].astype(str).values) + list(df_test[col].astype(str).values))\n",
    "        df_train[col] = le.transform(list(df_train[col].astype(str).values))\n",
    "        df_test[col] = le.transform(list(df_test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-7e9f88f07845>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#why sort ??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TransactionDT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isFraud'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TransactionDT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TransactionDT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isFraud'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TransactionDT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "#why sort ??\n",
    "X_train = df_train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y_train = df_train.sort_values('TransactionDT')['isFraud']\n",
    "\n",
    "X_test = df_test.drop(['TransactionDT'], axis=1)\n",
    "\n",
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20663"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "##The following will carry out automated parameters optimisation for mainly accuray and overfitting related parameters based on Bayesian optimisation\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Those parameters are the optimised ones for 4 hours \n",
    "#small min_data, large min_leaves, small learning rate, gbdt. \n",
    "#to check if scale_pos_weight can improve result for highly skewed data. \n",
    "\n",
    "best_param = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.0345,\n",
    "          'feature_fraction': 0.38,\n",
    "          'bagging_fraction': 0.418,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.00688,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.39,\n",
    "          'reg_lambda': 0.65,\n",
    "          'random_state': 47,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "Fold: 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.943657\tvalid_1's auc: 0.88207\n",
      "[200]\ttraining's auc: 0.956393\tvalid_1's auc: 0.890153\n",
      "[300]\ttraining's auc: 0.967593\tvalid_1's auc: 0.897283\n",
      "[400]\ttraining's auc: 0.977616\tvalid_1's auc: 0.902745\n",
      "[500]\ttraining's auc: 0.984579\tvalid_1's auc: 0.907301\n",
      "[600]\ttraining's auc: 0.989184\tvalid_1's auc: 0.910813\n",
      "[700]\ttraining's auc: 0.992486\tvalid_1's auc: 0.913438\n",
      "[800]\ttraining's auc: 0.994838\tvalid_1's auc: 0.91556\n",
      "[900]\ttraining's auc: 0.996459\tvalid_1's auc: 0.916916\n",
      "[1000]\ttraining's auc: 0.99762\tvalid_1's auc: 0.917975\n",
      "[1100]\ttraining's auc: 0.998385\tvalid_1's auc: 0.918777\n",
      "[1200]\ttraining's auc: 0.998907\tvalid_1's auc: 0.919396\n",
      "[1300]\ttraining's auc: 0.999246\tvalid_1's auc: 0.920039\n",
      "[1400]\ttraining's auc: 0.999476\tvalid_1's auc: 0.920141\n",
      "[1500]\ttraining's auc: 0.999628\tvalid_1's auc: 0.92046\n",
      "[1600]\ttraining's auc: 0.99974\tvalid_1's auc: 0.92043\n",
      "[1700]\ttraining's auc: 0.999822\tvalid_1's auc: 0.920533\n",
      "[1800]\ttraining's auc: 0.999879\tvalid_1's auc: 0.920542\n",
      "Early stopping, best iteration is:\n",
      "[1734]\ttraining's auc: 0.999844\tvalid_1's auc: 0.920612\n",
      "Fold 1 | AUC: 0.9206116161835491\n",
      "Fold: 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.942733\tvalid_1's auc: 0.900605\n",
      "[200]\ttraining's auc: 0.955213\tvalid_1's auc: 0.907643\n",
      "[300]\ttraining's auc: 0.966868\tvalid_1's auc: 0.914634\n",
      "[400]\ttraining's auc: 0.977391\tvalid_1's auc: 0.920924\n",
      "[500]\ttraining's auc: 0.984811\tvalid_1's auc: 0.925817\n",
      "[600]\ttraining's auc: 0.989773\tvalid_1's auc: 0.929223\n",
      "[700]\ttraining's auc: 0.993191\tvalid_1's auc: 0.931454\n",
      "[800]\ttraining's auc: 0.995544\tvalid_1's auc: 0.932994\n",
      "[900]\ttraining's auc: 0.997099\tvalid_1's auc: 0.934075\n",
      "[1000]\ttraining's auc: 0.998121\tvalid_1's auc: 0.934892\n",
      "[1100]\ttraining's auc: 0.99879\tvalid_1's auc: 0.935625\n",
      "[1200]\ttraining's auc: 0.999202\tvalid_1's auc: 0.936146\n",
      "[1300]\ttraining's auc: 0.999472\tvalid_1's auc: 0.936486\n",
      "[1400]\ttraining's auc: 0.99965\tvalid_1's auc: 0.936629\n",
      "[1500]\ttraining's auc: 0.999756\tvalid_1's auc: 0.93683\n",
      "[1600]\ttraining's auc: 0.99984\tvalid_1's auc: 0.936977\n",
      "[1700]\ttraining's auc: 0.999893\tvalid_1's auc: 0.937028\n",
      "[1800]\ttraining's auc: 0.999929\tvalid_1's auc: 0.937068\n",
      "Early stopping, best iteration is:\n",
      "[1758]\ttraining's auc: 0.999917\tvalid_1's auc: 0.937088\n",
      "Fold 2 | AUC: 0.9370890121908855\n",
      "Fold: 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.944008\tvalid_1's auc: 0.902716\n",
      "[200]\ttraining's auc: 0.956987\tvalid_1's auc: 0.91059\n",
      "[300]\ttraining's auc: 0.968511\tvalid_1's auc: 0.917226\n",
      "[400]\ttraining's auc: 0.978417\tvalid_1's auc: 0.923285\n",
      "[500]\ttraining's auc: 0.985457\tvalid_1's auc: 0.927999\n",
      "[600]\ttraining's auc: 0.990164\tvalid_1's auc: 0.931261\n",
      "[700]\ttraining's auc: 0.993391\tvalid_1's auc: 0.932971\n",
      "[800]\ttraining's auc: 0.99563\tvalid_1's auc: 0.934281\n",
      "[900]\ttraining's auc: 0.997138\tvalid_1's auc: 0.934991\n",
      "[1000]\ttraining's auc: 0.998138\tvalid_1's auc: 0.935383\n",
      "[1100]\ttraining's auc: 0.99879\tvalid_1's auc: 0.935664\n",
      "[1200]\ttraining's auc: 0.99921\tvalid_1's auc: 0.935839\n",
      "[1300]\ttraining's auc: 0.99948\tvalid_1's auc: 0.935755\n",
      "Early stopping, best iteration is:\n",
      "[1235]\ttraining's auc: 0.999319\tvalid_1's auc: 0.935853\n",
      "Fold 3 | AUC: 0.9358542724753778\n",
      "Fold: 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.941596\tvalid_1's auc: 0.917654\n",
      "[200]\ttraining's auc: 0.954663\tvalid_1's auc: 0.925228\n",
      "[300]\ttraining's auc: 0.966448\tvalid_1's auc: 0.932156\n",
      "[400]\ttraining's auc: 0.97705\tvalid_1's auc: 0.938525\n",
      "[500]\ttraining's auc: 0.984796\tvalid_1's auc: 0.943587\n",
      "[600]\ttraining's auc: 0.989725\tvalid_1's auc: 0.947072\n",
      "[700]\ttraining's auc: 0.993102\tvalid_1's auc: 0.948911\n",
      "[800]\ttraining's auc: 0.995481\tvalid_1's auc: 0.950212\n",
      "[900]\ttraining's auc: 0.997062\tvalid_1's auc: 0.950949\n",
      "[1000]\ttraining's auc: 0.998112\tvalid_1's auc: 0.951579\n",
      "[1100]\ttraining's auc: 0.99878\tvalid_1's auc: 0.952038\n",
      "[1200]\ttraining's auc: 0.999207\tvalid_1's auc: 0.952326\n",
      "[1300]\ttraining's auc: 0.999476\tvalid_1's auc: 0.952368\n",
      "[1400]\ttraining's auc: 0.999648\tvalid_1's auc: 0.952391\n",
      "[1500]\ttraining's auc: 0.99977\tvalid_1's auc: 0.952372\n",
      "Early stopping, best iteration is:\n",
      "[1405]\ttraining's auc: 0.999655\tvalid_1's auc: 0.952403\n",
      "Fold 4 | AUC: 0.9524026333994037\n",
      "Fold: 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.94365\tvalid_1's auc: 0.895334\n",
      "[200]\ttraining's auc: 0.955994\tvalid_1's auc: 0.902686\n",
      "[300]\ttraining's auc: 0.966859\tvalid_1's auc: 0.90982\n",
      "[400]\ttraining's auc: 0.977156\tvalid_1's auc: 0.91598\n",
      "[500]\ttraining's auc: 0.984762\tvalid_1's auc: 0.921212\n",
      "[600]\ttraining's auc: 0.989678\tvalid_1's auc: 0.924918\n",
      "[700]\ttraining's auc: 0.992989\tvalid_1's auc: 0.927063\n",
      "[800]\ttraining's auc: 0.995327\tvalid_1's auc: 0.928256\n",
      "[900]\ttraining's auc: 0.996901\tvalid_1's auc: 0.928872\n",
      "[1000]\ttraining's auc: 0.997971\tvalid_1's auc: 0.929332\n",
      "[1100]\ttraining's auc: 0.99868\tvalid_1's auc: 0.9296\n",
      "[1200]\ttraining's auc: 0.999124\tvalid_1's auc: 0.929724\n",
      "[1300]\ttraining's auc: 0.999422\tvalid_1's auc: 0.929681\n",
      "Early stopping, best iteration is:\n",
      "[1252]\ttraining's auc: 0.999291\tvalid_1's auc: 0.92978\n",
      "Fold 5 | AUC: 0.9297802817402705\n",
      "\n",
      "Mean AUC = 0.9351475631978973\n",
      "Out of folds AUC = 0.9354750736538122\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 5\n",
    "SEED = 6\n",
    "\n",
    "columns = X_train.columns\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "folds = KFold(n_splits=N_SPLITS, random_state=SEED)\n",
    "# Test Data and expport DF\n",
    "  \n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X_train.shape[0])\n",
    "\n",
    "score = 0\n",
    "y_result = 0\n",
    "print(\"running\")\n",
    "# use stratified fold to ensure the split datasets have same portion of postive and negative data.\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print('Fold:',fold_+1)\n",
    "    tr_x, tr_y = X_train.iloc[trn_idx,:], y_train.iloc[trn_idx]    \n",
    "    vl_x, v_y = X_train.iloc[val_idx,:], y_train.iloc[val_idx]    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)  \n",
    "        \n",
    "    #this will run very slow. \n",
    "    num_round =5000\n",
    "        \n",
    "    #here valid sets are the criteria for stopping, when the auc score for valid sets do not improve after num_round, then it will stop. \n",
    "    estimator = lgb.train(\n",
    "            best_param,\n",
    "            train_data, \n",
    "            num_round,\n",
    "            valid_sets = [train_data,valid_data],\n",
    "            #print evaluation at each step\n",
    "            verbose_eval = 100,\n",
    "            early_stopping_rounds = 100\n",
    "            )\n",
    "\n",
    "    y_pred_valid = estimator.predict(vl_x)\n",
    "    y_oof[val_idx] = y_pred_valid\n",
    "    print(f\"Fold {fold_ + 1} | AUC: {roc_auc_score(v_y, y_pred_valid)}\")\n",
    "    \n",
    "    #score is the roc_auc of valid pred \n",
    "    score += roc_auc_score(v_y, y_pred_valid) / N_SPLITS\n",
    "    y_result += estimator.predict(X_test) / N_SPLITS\n",
    "    # we are not sure what fold is best for us\n",
    "    # so we will average prediction results \n",
    "    # over folds; meaning each fold we predict test data and then get average.\n",
    "    \n",
    "    feature_importances[f'fold_{fold_ + 1}'] = estimator.feature_importance()\n",
    "\n",
    "    del train_data, valid_data\n",
    "    gc.collect()\n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y_train, y_oof)}\")\n",
    "    #print('LOG loss', metrics.log_loss(RESULTS['isFraud'], RESULTS['stratifiedkfold']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##take the mean of each feature importance of all folds. \n",
    "feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18537"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sub = pd.DataFrame(columns=['TransactionID','isFraud'])\n",
    "sub['TransactionID'] = test_trans_id\n",
    "sub['isFraud'] = y_result\n",
    "sub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sub.to_csv(\"submission29th.csv\",index=False)\n",
    "\n",
    "sum(y_result>0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
